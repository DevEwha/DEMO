"""
Progressive LLM Loading Demo
ìºì‹œ ì •ë¦¬: sudo sync; echo 1 | sudo tee /proc/sys/vm/drop_caches
"""

from glob import glob
import os
import torch
import json
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM
from dataclasses import dataclass
from typing import List
from transformers.models.llama.modeling_llama import LlamaDecoderLayer
from safetensors.torch import load_file
from transformers import AutoConfig, AutoModelForCausalLM, pipeline
from transformers.modeling_utils import load_sharded_checkpoint
from peft import LoraConfig, get_peft_model, PeftModel

from model_utils import *


@dataclass
class Config:
    """ì„¤ì • í´ë˜ìŠ¤"""
    base_dir: str = "./models/25_pruning_AB_lora"
    device: str = "cuda:0"
    max_length: int = 150
    temperature: float = 0.7
    seqlen: int = 4096

    @property
    def a_dir(self) -> str:
        return f"{self.base_dir}/A"
    
    @property
    def b_dir(self) -> str:
        return f"{self.base_dir}/bundles/B"
    
    @property
    def c_dir(self) -> str:
        return f"{self.base_dir}/bundles/C"
    
    @property
    def adapter_dir(self) -> str:
        return f"{self.base_dir}/adapters"
    
    @property
    def a_adapter_config_path(self) -> str:
        return os.path.join(self.adapter_dir, "A_lora", "stageA", "adapter_config.json")
    
    @property
    def a_adapter_pt_path(self) -> str:
        return os.path.join(self.adapter_dir, "A_lora", "stageA.pt")
    
    @property
    def ab_adapter_config_path(self) -> str:
        return os.path.join(self.adapter_dir, "AB_lora", "stageAB", "adapter_config.json")
    
    @property
    def ab_adapter_pt_path(self) -> str:
        return os.path.join(self.adapter_dir, "AB_lora", "stageAB.pt")
    
    @property
    def abc_adapter_config_path(self) -> str:
        return os.path.join(self.adapter_dir, "ABC_lora", "stageABC", "adapter_config.json")
    
    @property
    def abc_adapter_pt_path(self) -> str:
        return os.path.join(self.adapter_dir, "ABC_lora", "stageABC.pt")


def print_header(text: str, char: str = "="):
    """ê¹”ë”í•œ í—¤ë” ì¶œë ¥"""
    width = 80
    print(f"\n{char * width}")
    print(f"{text.center(width)}")
    print(f"{char * width}\n")


def print_step(step_num: int, text: str):
    """ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ì¶œë ¥"""
    print(f"\n{'â–¶'*3} STAGE {step_num}: {text}")


def print_info(text: str):
    """ì •ë³´ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"  â„¹ï¸  {text}")


def print_success(text: str):
    """ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥"""
    print(f"  âœ… {text}")


def log_mem(tag=""):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
    alloc = torch.cuda.memory_allocated() / 1e9
    reserv = torch.cuda.memory_reserved() / 1e9
    print(f"  ğŸ’¾ {tag}: GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ = {alloc:.2f}GB / {reserv:.2f}GB")


def free_cuda(*objs):
    """ì°¸ì¡° í•´ì œ ë° ìºì‹œ ë¹„ìš°ê¸°"""
    for o in objs:
        try:
            del o
        except Exception:
            pass
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()


def generate_sample(model, tokenizer, stage_name: str, prompt: str, max_new_tokens: int = 50):
    """ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìƒì„±"""
    try:
        model.eval()
        with torch.no_grad():
            inputs = tokenizer(prompt, return_tensors="pt", padding=True)
            input_ids = inputs["input_ids"].to(model.device)
            attention_mask = inputs.get("attention_mask", None)
            if attention_mask is not None:
                attention_mask = attention_mask.to(model.device)
            
            print_info("í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            print(f"\n{'â”€'*80}")
            print(f"ğŸ“ [{stage_name}] ìƒì„±ëœ í…ìŠ¤íŠ¸:")
            print(f"{'â”€'*80}")
            print(f"{generated_text}")
            print(f"{'â”€'*80}\n")
            
    except Exception as e:
        print(f"  âŒ ìƒì„± ì‹¤íŒ¨ ({stage_name}): {e}")


def main():
    print_header("ğŸš€ Progressive LLM Loading Demo", "=")
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    print("\n" + "="*80)
    user_prompt = input("ğŸ¤ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not user_prompt:
        user_prompt = "The future of artificial intelligence is"
        print_info(f"ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {user_prompt}")
    print("\n" + "="*80)
    
    config = Config()
    print_info(f"ë””ë°”ì´ìŠ¤: {config.device}")
    print_info(f"ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬: {config.base_dir}")
    
    device = config.device if torch.cuda.is_available() else "cpu"
    log_mem("ì´ˆê¸° ìƒíƒœ")
    
    # ==================== STAGE 1: ëª¨ë¸ A ë¡œë”© ====================
    print_step(1, "ëª¨ë¸ A ë¡œë”© (Pruned Model)")
    
    print_info("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(str(config.a_dir))
    print_success("í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
    
    print_info("ëª¨ë¸ A ë¡œë”© ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        config.a_dir,
        torch_dtype=torch.float16,
        device_map="cuda:0"
    )
    model.config.use_cache = False
    model.to(device)
    print_success("ëª¨ë¸ A ë¡œë”© ì™„ë£Œ")
    log_mem("ëª¨ë¸ A ë¡œë”© í›„")
    
    print_info("PassLayer ì ìš© ì¤‘...")
    manifest_path = os.path.join(config.a_dir, "manifest.json")
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    removed = find_removed_layers(manifest)
    model = install_pass_layers(
        model,
        removed_indices=removed,
        get_layer_container=get_layer_container,
        get_layer_device=lambda layer: torch.device("cuda:0"),
        default_device=torch.device("cuda:0"),
    )
    print_success("PassLayer ì ìš© ì™„ë£Œ")
    
    print_info("Stage A ì–´ëŒ‘í„° ì ìš© ì¤‘...")
    adapter_config = json.load(open(config.a_adapter_config_path, "r", encoding="utf-8"))
    lora_config = LoraConfig(
        task_type=adapter_config.get("task_type", "CAUSAL_LM"),
        r=adapter_config.get("r", 16),
        lora_alpha=adapter_config.get("lora_alpha", 32),
        target_modules=adapter_config.get("target_modules", ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]),
        lora_dropout=adapter_config.get("lora_dropout", 0.05),
        bias=adapter_config.get("bias", "none"),
        inference_mode=False
    )
    model = get_peft_model(model, lora_config)
    
    checkpoint = torch.load(config.a_adapter_pt_path, map_location=model.device)
    if isinstance(checkpoint, dict):
        state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
    else:
        state_dict = checkpoint
    
    fixed_state_dict = fix_parameter_keys(state_dict, model)
    missing_keys, unexpected_keys = model.load_state_dict(fixed_state_dict, strict=False)
    
    lora_loaded = len([k for k in fixed_state_dict.keys() if 'lora_' in k])
    print_success(f"Stage A ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ (LoRA íŒŒë¼ë¯¸í„°: {lora_loaded}ê°œ)")
    log_mem("ì–´ëŒ‘í„° A ì ìš© í›„")
    
    generate_sample(model, tokenizer, "Stage 1: Model A", user_prompt, max_new_tokens=50)
    
    # ==================== STAGE 2: ëª¨ë¸ A+B ë¡œë”© ====================
    print_step(2, "ëª¨ë¸ B ë³µêµ¬ (A + B)")
    
    print_info("prune_log.json ì½ëŠ” ì¤‘...")
    path = os.path.join(config.a_dir, "prune_log.json")
    with open(path, "r", encoding="utf-8") as f:
        log = json.load(f)
    B_idx, C_idx = log["split"]["B"], log["split"]["C"]
    print_info(f"ë ˆì´ì–´ ì¸ë±ìŠ¤ - B: {len(B_idx)}ê°œ, C: {len(C_idx)}ê°œ")
    
    print_info("B ë ˆì´ì–´ ë³µêµ¬ ì¤‘...")
    rehydrate_layers(model, config.b_dir, B_idx)
    print_success("B ë ˆì´ì–´ ë³µêµ¬ ì™„ë£Œ")
    
    print_info("Stage AB ì–´ëŒ‘í„° ì ìš© ì¤‘...")
    adapter_config = json.load(open(config.ab_adapter_config_path, "r", encoding="utf-8"))
    lora_config = LoraConfig(
        task_type=adapter_config.get("task_type", "CAUSAL_LM"),
        r=adapter_config.get("r", 16),
        lora_alpha=adapter_config.get("lora_alpha", 32),
        target_modules=adapter_config.get("target_modules", ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]),
        lora_dropout=adapter_config.get("lora_dropout", 0.05),
        bias=adapter_config.get("bias", "none"),
        inference_mode=False
    )
    
    adapter_name = "adapter_AB"
    model.add_adapter(peft_config=lora_config, adapter_name=adapter_name)
    model.set_adapter(adapter_name)
    
    checkpoint = torch.load(config.ab_adapter_pt_path, map_location=model.device)
    if isinstance(checkpoint, dict):
        state_dict = checkpoint.get('model', checkpoint.get('state_dict', checkpoint))
    else:
        state_dict = checkpoint
    
    fixed_state_dict = fix_parameter_keys(state_dict, model)
    missing_keys, unexpected_keys = model.load_state_dict(fixed_state_dict, strict=False)
    print_success("Stage AB ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ")
    log_mem("ì–´ëŒ‘í„° AB ì ìš© í›„")
    
    generate_sample(model, tokenizer, "Stage 2: Model A+B", user_prompt, max_new_tokens=50)
    
    # ==================== STAGE 3: ì „ì²´ ëª¨ë¸ (A+B+C) ====================
    print_step(3, "ëª¨ë¸ C ë³µêµ¬ (A + B + C - ì „ì²´ ëª¨ë¸)")
    
    print_info("C ë ˆì´ì–´ ë³µêµ¬ ì¤‘...")
    rehydrate_layers(model, config.c_dir, C_idx)
    print_success("C ë ˆì´ì–´ ë³µêµ¬ ì™„ë£Œ")
    log_mem("ì „ì²´ ëª¨ë¸ ë³µêµ¬ í›„")
    
    generate_sample(model, tokenizer, "Stage 3: Full Model (A+B+C)", user_prompt, max_new_tokens=50)
    
    print_header("âœ¨ ë°ëª¨ ì™„ë£Œ", "=")


if __name__ == "__main__":
    main()
