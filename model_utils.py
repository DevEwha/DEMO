import torch
from safetensors.torch import load_file
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
from pathlib import Path
from typing import Dict
from typing import Optional, List, Any
from transformers.models.llama.modeling_llama import LlamaDecoderLayer
import os

def safe_to_device(model: torch.nn.Module, device: str):
    """ë©”íƒ€ ë””ë°”ì´ìŠ¤ì—ì„œ ì•ˆì „í•˜ê²Œ ë””ë°”ì´ìŠ¤ ì´ë™"""
    if hasattr(model, 'to_empty'):
        return model.to_empty(device=device)
    else:
        return model.to(device)

def load_model_config(model_path: Path):
    """ëª¨ë¸ ì„¤ì • ë¡œë”©"""
    config_dir = model_path / "original_config"
    if not config_dir.exists():
        raise FileNotFoundError(f"ì„¤ì • í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {config_dir}")
    return AutoConfig.from_pretrained(str(config_dir))

def load_model_tokenizer(model_path: Path):
    """í† í¬ë‚˜ì´ì € ë¡œë”©"""
    config_dir = model_path / "original_config"
    return AutoTokenizer.from_pretrained(str(config_dir))

def load_manifest(model_path: Path):
    """manifest.json ë¡œë”©"""
    import json
    manifest_path = model_path / "manifest.json"
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    p_layers = set(manifest['blocks']['P']['layers'])
    r1_layers = set(manifest['blocks']['R1']['layers'])
    r2_layers = set(manifest['blocks']['R2']['layers'])
    
    return manifest, p_layers, r1_layers, r2_layers

def load_safetensors_file(model_path: Path, filename: str, target_device: str = "cpu") -> Dict[str, torch.Tensor]:
    """SafeTensors íŒŒì¼ ë¡œë”©"""
    file_path = model_path / filename
    if not file_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    state_dict = load_file(str(file_path))
    
    if target_device != "cpu":
        state_dict = {k: v.to(target_device) for k, v in state_dict.items()}
    
    return state_dict

def create_zero_initialized_model(config, use_meta_device_first=True):
    """0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ëª¨ë¸ ìƒì„±"""
    use_meta_device = False
    
    if use_meta_device_first:
        try:
            with torch.device("meta"):
                model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)
            use_meta_device = True
        except Exception:
            model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)
            use_meta_device = False
    else:
        model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)
        use_meta_device = False
    
    # 0ìœ¼ë¡œ ì´ˆê¸°í™”
    if not use_meta_device:
        with torch.no_grad():
            for param in model.parameters():
                if param.requires_grad:
                    param.zero_()
    
    return model, use_meta_device

def apply_state_dict_safe(model, state_dict, use_meta_device=False):
    """ì•ˆì „í•œ state_dict ì ìš©"""
    if use_meta_device:
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False, assign=True)
    else:
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    
    return missing_keys, unexpected_keys

def simple_inference_test(model, tokenizer, device="cpu", text="Hello, this is a test."):
    """ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    if model is None:
        return None, None
    
    model_device = next(model.parameters()).device
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(model_device) for k, v in inputs.items()}
    
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        generated = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
            temperature=1.0,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )
    
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    return generated_text, outputs

#ë‚˜ê²½ì´ ì½”ë“œì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜
def find_removed_layers(manifest: dict) -> Optional[List[int]]:
    """ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì—ì„œ ì œê±°ëœ ë ˆì´ì–´ ì°¾ê¸°""" # simdrop ìŠ¤í‚¤ë§ˆ
    removed = manifest.get("simdrop", {}).get("removed_layers")
    if removed:
        return removed
            
    # top-level ìŠ¤í‚¤ë§ˆ
    removed = manifest.get("removed_layers")
    if removed:
        return removed
            
    # stages ìŠ¤í‚¤ë§ˆ
    stages = manifest.get("stages", {})
    if stages:
        A_drop = stages.get("A", {}).get("dropped_layers", [])
        B_rem = stages.get("B", {}).get("removed_layers", [])
        C_rem = stages.get("C", {}).get("removed_layers", [])
        return A_drop or sorted(set(B_rem + C_rem))
            
    return None


# utils/pass_layers.py
from typing import List, Callable, Optional
import torch.nn as nn

def install_pass_layers(
    model,
    removed_indices: List[int],
    get_layer_container: Callable,     # fn(model) -> list-like container or None
    get_layer_device: Callable,        # fn(module) -> device or None
    default_device: Optional[str] = None,
):
    # PassLayer êµ¬í˜„ ì„ íƒ
    try:
        from lib.identity import LlamaPassLayer as _Inner
        class PassWrapper(nn.Module):
            def __init__(self, hidden_size):
                super().__init__()
                self.inner = _Inner(hidden_size)
            def forward(self, hidden_states, *args, **kwargs):
                out = self.inner(hidden_states, *args, **kwargs)
                return out[0] if isinstance(out, tuple) else out
        print("[reapply] using project LlamaPassLayer")
    except Exception:
        class PassWrapper(nn.Module):
            def __init__(self, hidden_size):
                super().__init__()
            def forward(self, x, *args, **kwargs):
                return x
        print("[reapply] using SafePassLayer")

    # ë ˆì´ì–´ ì»¨í…Œì´ë„ˆ
    layer_container = get_layer_container(model)
    if layer_container is None:
        print("[reapply] cannot locate layers -> skip")
        return model

    hidden_size = getattr(model.config, "hidden_size", None)
    if hidden_size is None:
        raise ValueError("model.config.hidden_sizeê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    applied = []
    for i in removed_indices:
        if 0 <= i < len(layer_container):
            dev = get_layer_device(layer_container[i]) or default_device
            wrapper = PassWrapper(hidden_size)
            if dev is not None:
                wrapper = wrapper.to(dev)
            layer_container[i] = wrapper
            applied.append(i)

    print(f"[reapply] installed PassLayer on: {applied}")
    return model


CANDIDATE_LAYER_PATHS = [
        "model.layers",
        "model.decoder.layers",
        "model.model.layers",
        "model.model.decoder.layers",
        "base_model.model.layers",
        "base_model.model.decoder.layers",
        "base_model.model.model.layers",
        "base_model.model.model.decoder.layers",
]

def get_layer_container(model) -> Optional[object]:
    for path in CANDIDATE_LAYER_PATHS:
        try:
            container = model
            for attr in path.split("."):
                container = getattr(container, attr)
            if hasattr(container, "__len__") and hasattr(container, "__getitem__"):
                return container
        except AttributeError:
            continue
    return None
    

def rehydrate_layers(model, bundle_dir: str, indices: List[int]):
    
    
    layers = get_layer_container(model)
    dtype = next(model.parameters()).dtype
    tgt = next(model.parameters()).device   # â† ë‹¨ì¼ ë””ë°”ì´ìŠ¤ë¡œ ê³ ì •


    for i in indices:
        new_layer = LlamaDecoderLayer(model.config, layer_idx=i).to(device=tgt, dtype=dtype)
        f = os.path.join(bundle_dir, f"layer_{i:03d}.safetensors")
        if not os.path.isfile(f):
            raise FileNotFoundError(f"bundle miss: {f}")
        sd = load_file(f)
        sd = {k: v.to(device=tgt, dtype=dtype) for k, v in sd.items()}
        try:
            new_layer.load_state_dict(sd, strict=True)
        except RuntimeError as e:
            print(f"[warn] strict load failed for {i}: {e} -> non-strict")
            new_layer.load_state_dict(sd, strict=False)
        layers[i] = new_layer
        print(f"[rehydrate] layer {i} restored on {tgt}")






# peft ê´€ë ¨ ìœ í‹¸

def fix_parameter_keys(
    state_dict: Dict[str, Any],
    model: Optional[object] = None,  # PeftModelì´ ì•„ë‹ˆì–´ë„ ë™ì‘í•˜ë„ë¡ ì™„í™”
) -> Dict[str, Any]:
    """
    LoRA state_dictì˜ í‚¤ë¥¼ í˜„ì¬ ì–´ëŒ‘í„° ì´ë¦„ì— ë§ê²Œ ë¦¬ë§µí•œë‹¤.
    modelì´ PeftModelì´ë©´ peft_configì—ì„œ ì–´ëŒ‘í„° ì´ë¦„ì„ ì¶”ì¶œí•œë‹¤.
    """
    adapter_name = "default"
    # modelì´ PeftModelì¸ ê²½ìš° ì–´ëŒ‘í„° ì´ë¦„ ì¶”ì¶œ
    if model is not None and hasattr(model, "peft_config"):
        names = list(getattr(model, "peft_config").keys())
        if names:
            adapter_name = names[0]

    fixed = {}
    for key, value in state_dict.items():
        if "lora_A.weight" in key:
            new_key = key.replace("lora_A.weight", f"lora_A.{adapter_name}.weight")
        elif "lora_B.weight" in key:
            new_key = key.replace("lora_B.weight", f"lora_B.{adapter_name}.weight")
        elif "lora_C.weight" in key:
            new_key = key.replace("lora_C.weight", f"lora_C.{adapter_name}.weight")
        else:
            new_key = key
        fixed[new_key] = value
    return fixed


from typing import Optional
import torch


# ì—¬ê¸° í•¨ìˆ˜ ë„ˆë¬´ ê¸´ë° ìµœì í™” í•„ìš” 
"""
def check_activation(model) -> bool:
    
    LoRA ì–´ëŒ‘í„° í™œì„± ìƒíƒœë¥¼ ì ê²€í•œë‹¤.
    - PeftModel ì—¬ë¶€ í™•ì¸
    - lora_ íŒŒë¼ë¯¸í„° ì¡´ì¬ ìˆ˜ ì§‘ê³„
    - ì•ˆì „í•œ ë”ë¯¸ í† í° ì…ë ¥ìœ¼ë¡œ 1íšŒ forwardí•˜ì—¬ LoRA ê³„ì¸µ í›… íŠ¸ë¦¬ê±° í™•ì¸
    
    print("ğŸ” Checking adapter activation...")

    # 1) PeftModel í™•ì¸(ë¬¸ìì—´ ë¹„êµë¡œ ì˜ì¡´ ì™„í™”)
    cls_name = type(model).__name__
    if "PeftModel" not in cls_name:
        print("   âŒ Not a PEFT model")
        return False

    # 2) LoRA íŒŒë¼ë¯¸í„° ìˆ˜
    try:
        lora_count = sum(1 for n, _ in model.named_parameters() if "lora_" in n)
    except Exception as e:
        print(f"   âŒ Failed to enumerate parameters: {e}")
        return False
    print(f"   ğŸ“Š LoRA parameters found: {lora_count}")
    if lora_count == 0:
        return False

    # 3) í›…ìœ¼ë¡œ í™œì„± ê³„ì¸µ ê°ì§€
    activations = []
    hooks = []

    def hook_fn(module, inputs, output):
        # LoRA ì£¼ì…ì‹œ ëª¨ë“ˆì— lora_A ë“±ì´ ì†ì„±ìœ¼ë¡œ ë¶™ìŒ [web:3][web:174]
        if hasattr(module, "lora_A"):
            activations.append(type(module).__name__)

    try:
        for name, module in model.named_modules():
            # adapter ì´ë¦„ì´ defaultì¸ ì¼€ì´ìŠ¤ë¥¼ ìš°ì„  ê°€ì •, ë‹¤ë¥¸ ì´ë¦„ì´ì–´ë„ lora_A ì†ì„±ì€ ì¡´ì¬ [web:3]
            if hasattr(module, "lora_A"):
                hooks.append(module.register_forward_hook(hook_fn))

        # 4) ë”ë¯¸ ì…ë ¥ ìƒì„±: causal LMìš© ìµœì†Œ ì…ë ¥
        device = getattr(model, "device", None)
        if device is None:
            # PeftModelì€ base_modelì˜ íŒŒë¼ë¯¸í„° ë””ë°”ì´ìŠ¤ë¥¼ ë”°ë¥¸ë‹¤ [web:174]
            try:
                device = next(model.parameters()).device
            except StopIteration:
                device = torch.device("cpu")

        dummy = torch.randint(0, 1000, (1, 5), device=device)  # ê°„ë‹¨ í† í° ì‹œí€€ìŠ¤
        with torch.no_grad():
            # ì¼ë¶€ ëª¨ë¸ì€ í‚¤ì›Œë“œ ì…ë ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹œë„ ìˆœì„œë¥¼ ë‘”ë‹¤
            try:
                model(dummy)  # input_ids í…ì„œ ìœ„ì¹˜ ì¸ì ê²½ë¡œ
            except Exception:
                model(input_ids=dummy)  # í‚¤ì›Œë“œ ì¸ì ê²½ë¡œ

        activated = len(activations) > 0
        print(f"   {'âœ…' if activated else 'âŒ'} LoRA layers activated: {len(activations)}")
        return activated

    except Exception as e:
        print(f"   âŒ Activation test failed: {e}")
        return False

    finally:
        for h in hooks:
            try:
                h.remove()
            except Exception:
                pass
"""

def check_activation(model) -> bool:
    """LoRA ì–´ëŒ‘í„° í™œì„± í™•ì¸ - ì•ˆì „ ë²„ì „"""
    print("ğŸ” Checking adapter activation...")
    
    # PeftModel í™•ì¸
    if "PeftModel" not in type(model).__name__:
        print("   âŒ Not a PEFT model")
        return False
    
    # LoRA íŒŒë¼ë¯¸í„° í™•ì¸ë§Œìœ¼ë¡œ ì¶©ë¶„
    try:
        lora_count = sum(1 for n, _ in model.named_parameters() if "lora_" in n)
        print(f"   ğŸ“Š LoRA parameters found: {lora_count}")
        
        if lora_count > 0:
            print(f"   âœ… LoRA adapter is active")
            return True
        else:
            print(f"   âŒ No LoRA parameters")
            return False
            
    except Exception as e:
        print(f"   âŒ Error: {e}")
        return False